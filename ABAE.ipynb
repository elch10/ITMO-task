{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LOovQv86mBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udm_sAxw6njo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "!unzip /gdrive/My\\ Drive/task5.zip -d . >> /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OZu_6ep6laC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e4610847-e76a-4e0e-9d08-a7503ffcad7f"
      },
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "PATH = 'Задача 1/input/'\n",
        "\n",
        "df = pd.read_csv(PATH + 'train.csv', index_col='doc_id')\n",
        "test_df = pd.read_csv('Задача 1/input/test.csv', index_col='doc_id')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13.5 s, sys: 1.41 s, total: 14.9 s\n",
            "Wall time: 14.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B5EYzThq1ac",
        "colab_type": "text"
      },
      "source": [
        "## Attention Based Aspect Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KSCMDRmHYog",
        "colab_type": "text"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H03zaeaC2Xct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.layers import Dense, Activation, Embedding, Input\n",
        "from keras.models import Model\n",
        "from my_layers import Attention, Average, WeightedSum, WeightedAspectEmb, MaxMargin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6fan9Oy2Jmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(sentence_len, vocab_size, emb_size, aspect_size, \n",
        "                 neg_samp_size, reg_term):\n",
        "    def ortho_reg(weight_matrix):\n",
        "        ### orthogonal regularization for aspect embedding matrix ###\n",
        "        w_n = weight_matrix / K.cast(K.epsilon() + \n",
        "                                     K.sqrt(K.sum(K.square(weight_matrix), \n",
        "                                                  axis=-1, keepdims=True)), \n",
        "                                     K.floatx())\n",
        "        reg = K.sum(K.square(K.dot(w_n, K.transpose(w_n)) - K.eye(aspect_size)))\n",
        "        return reg_term*reg\n",
        "\n",
        "    ##### Inputs #####\n",
        "    sentence_input = Input(shape=(sentence_len,), dtype='int32', name='sentence_input')\n",
        "    neg_input = Input(shape=(neg_samp_size, sentence_len), dtype='int32', name='neg_input')\n",
        "\n",
        "    ##### Construct word embedding layer #####\n",
        "    word_emb = Embedding(vocab_size, emb_size, mask_zero=True, name='word_emb')\n",
        "\n",
        "    ##### Compute sentence representation #####\n",
        "    e_w = word_emb(sentence_input)\n",
        "    y_s = Average()(e_w)\n",
        "    att_weights = Attention(name='att_weights')([e_w, y_s])\n",
        "    z_s = WeightedSum()([e_w, att_weights])\n",
        "\n",
        "    ##### Compute representations of negative instances #####\n",
        "    e_neg = word_emb(neg_input)\n",
        "    z_n = Average()(e_neg)\n",
        "\n",
        "    ##### Reconstruction #####\n",
        "    p_t = Dense(aspect_size)(z_s)\n",
        "    p_t = Activation('softmax', name='p_t')(p_t)\n",
        "    r_s = WeightedAspectEmb(aspect_size, emb_size, name='aspect_emb',\n",
        "            W_regularizer=ortho_reg)(p_t)\n",
        "\n",
        "    ##### Loss #####\n",
        "    loss = MaxMargin(name='max_margin')([z_s, z_n, r_s])\n",
        "    model = Model(input=[sentence_input, neg_input], output=loss)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQYJGMZX7ZOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_batch_generator(data, batch_size, sentence_len):\n",
        "    n_batch = len(data) // batch_size\n",
        "    batch_count = 0\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    while True:\n",
        "        if batch_count == n_batch:\n",
        "            np.random.shuffle(data)\n",
        "            batch_count = 0\n",
        "\n",
        "        batch = data[batch_count*batch_size: (batch_count+1)*batch_size]\n",
        "        batch = pad_sequences(batch, maxlen=sentence_len)\n",
        "        batch_count += 1\n",
        "        yield batch\n",
        "\n",
        "def negative_batch_generator(data, batch_size, sentence_len, neg_size):\n",
        "    data_len = len(data)\n",
        "\n",
        "    while True:\n",
        "        indices = np.random.choice(data_len, batch_size * neg_size)\n",
        "        samples = pad_sequences(np.take(data, indices), maxlen=sentence_len)\n",
        "        samples = samples.reshape(batch_size, neg_size, sentence_len)\n",
        "        yield samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuGQ7c4DHcs8",
        "colab_type": "text"
      },
      "source": [
        "### Приведение данных к нужному виду"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noqd-3lcasLt",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на длины документов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meZbgex6ZK18",
        "colab_type": "code",
        "outputId": "773253ec-cde9-4cc6-c8e7-c5ac89f4c3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df['word_amount'] = df['text'].str.split(' ').apply(len)\n",
        "df['word_amount'].describe(percentiles=[.25, .5, .75, .8, .9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    73987.000000 \n",
              "mean     812.827916   \n",
              "std      2718.921237  \n",
              "min      101.000000   \n",
              "25%      153.000000   \n",
              "50%      257.000000   \n",
              "75%      558.000000   \n",
              "80%      707.800000   \n",
              "90%      1421.000000  \n",
              "max      155410.000000\n",
              "Name: word_amount, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CryRoeNwbClV",
        "colab_type": "text"
      },
      "source": [
        "Чтобы было возможно обучаться, ограничим длину 1500 - это больше 90% датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmOmU7oo7wQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sentence_len = 1500\n",
        "train_df = df.loc[df['word_amount'] < max_sentence_len, 'text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snYS22hn013g",
        "colab_type": "code",
        "outputId": "a4f759ce-6d61-4e4a-dcf3-de4ca191e88d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 50000\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 52.3 s, sys: 359 ms, total: 52.7 s\n",
            "Wall time: 52.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A17XM9IL9SL6",
        "colab_type": "code",
        "outputId": "20293f40-97b1-4cfb-dda1-973b48b49c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "encoded = tokenizer.texts_to_sequences(train_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 40.9 s, sys: 34 ms, total: 41 s\n",
            "Wall time: 41 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJsLF-IHiJu",
        "colab_type": "text"
      },
      "source": [
        "### Создание модели и обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APEHSOpSBwH2",
        "colab_type": "text"
      },
      "source": [
        "Попробуем создать 10 топиков, как и в прошлый раз"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1CHUrk8509Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_size = 100\n",
        "aspect_size = 10\n",
        "neg_samp_size = 10\n",
        "reg_term = 0.01\n",
        "model = create_model(sentence_len=max_sentence_len, vocab_size=vocab_size, \n",
        "                     emb_size=emb_size, aspect_size=aspect_size, \n",
        "                     neg_samp_size=neg_samp_size, reg_term=reg_term)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se95aRdm61IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import adam\n",
        "\n",
        "def max_margin_loss(y_true, y_pred):\n",
        "    return K.mean(y_pred)\n",
        "  \n",
        "\n",
        "model.compile(optimizer=adam(lr=0.01, clipvalue=1), \n",
        "              loss=max_margin_loss, metrics=[max_margin_loss])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTj6TI_NUuqd",
        "colab_type": "code",
        "outputId": "7d81f749-d342-4dba-a639-df8664b195eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "sentence_input (InputLayer)     (None, 1500)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_emb (Embedding)            multiple             5000000     sentence_input[0][0]             \n",
            "                                                                 neg_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_9 (Average)             (None, 1500, 100)    0           word_emb[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "att_weights (Attention)         [(None, 1500, 100),  10001       word_emb[0][0]                   \n",
            "                                                                 average_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "weighted_sum_5 (WeightedSum)    [(None, 1500, 100),  0           word_emb[0][0]                   \n",
            "                                                                 att_weights[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "neg_input (InputLayer)          (None, 10, 1500)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1500, 10)     1010        weighted_sum_5[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "p_t (Activation)                (None, 1500, 10)     0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "average_10 (Average)            (None, 10, 1500, 100 0           word_emb[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "aspect_emb (WeightedAspectEmb)  (None, 1500, 10)     1000        p_t[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "max_margin (MaxMargin)          [(None, 1500, 100),  0           weighted_sum_5[0][0]             \n",
            "                                                                 average_10[0][0]                 \n",
            "                                                                 aspect_emb[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 5,012,011\n",
            "Trainable params: 5,012,011\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW_S7mnbQnrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "def data_generator():\n",
        "  sen_gen = sentence_batch_generator(encoded, batch_size, max_sentence_len)\n",
        "  neg_gen = negative_batch_generator(encoded, batch_size, max_sentence_len, neg_samp_size)\n",
        "  \n",
        "  while True:\n",
        "    sen_inp = next(sen_gen)\n",
        "    neg_inp = next(neg_gen)\n",
        "    yield ([sen_inp, neg_inp], np.ones((batch_size, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP_6M2cYcpNV",
        "colab_type": "code",
        "outputId": "8259a2fe-fd92-4176-ec9c-fffaa48e3e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "steps_per_epoch = 10\n",
        "epochs = 15\n",
        "\n",
        "checkpoint = ModelCheckpoint('best.h5', monitor='max_margin_loss', save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "history = model.fit_generator(generator=data_generator(),\n",
        "                              steps_per_epoch=steps_per_epoch,\n",
        "                              epochs=epochs, callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "10/10 [==============================] - 29s 3s/step - loss: 10.0332 - max_margin_loss: 9.9503\n",
            "Epoch 2/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 8.9717 - max_margin_loss: 8.7849\n",
            "Epoch 3/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 7.1327 - max_margin_loss: 6.8508\n",
            "Epoch 4/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.2408 - max_margin_loss: 4.8490\n",
            "Epoch 5/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 2.0356 - max_margin_loss: 1.6525\n",
            "Epoch 6/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.7711 - max_margin_loss: 0.4360\n",
            "Epoch 7/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.6362 - max_margin_loss: 0.3637\n",
            "Epoch 8/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.5256 - max_margin_loss: 0.3169\n",
            "Epoch 9/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.2409 - max_margin_loss: 0.0880\n",
            "Epoch 10/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.3436 - max_margin_loss: 0.1421\n",
            "Epoch 11/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.5362 - max_margin_loss: 0.2772\n",
            "Epoch 12/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.3835 - max_margin_loss: 0.1303\n",
            "Epoch 13/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.4405 - max_margin_loss: 0.2010\n",
            "Epoch 14/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.5933 - max_margin_loss: 0.2545\n",
            "Epoch 15/15\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.7497 - max_margin_loss: 0.2903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7UorD-mej_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp best.h5 /gdrive/My\\ Drive/ITMO/ABAE.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-nGOfY4Hr-4",
        "colab_type": "text"
      },
      "source": [
        "#### Результат"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8-SfkUrXq5X",
        "colab_type": "code",
        "outputId": "619e8b45-bdd5-4aa2-9aee-da001572066b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model.load_weights('/gdrive/My\\ Drive/ITMO/ABAE.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 18:26:58.835253 140502895511424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0723 18:26:58.837038 140502895511424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eosc9xvtHLBD",
        "colab_type": "text"
      },
      "source": [
        "Взглянем на топ слов, которые соответсвуют каждому топику"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG9VkCcE-IDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_emb = model.get_layer('word_emb').get_weights()[0]\n",
        "aspect_emb = model.get_layer('aspect_emb').get_weights()[0]\n",
        "word_emb = word_emb / np.linalg.norm(word_emb, axis=-1, keepdims=True)\n",
        "aspect_emb = aspect_emb / np.linalg.norm(aspect_emb, axis=-1, keepdims=True)\n",
        "\n",
        "topics_to_words = []\n",
        "\n",
        "for ind in range(len(aspect_emb)):\n",
        "    desc = aspect_emb[ind]\n",
        "    sims = word_emb.dot(desc.T)\n",
        "    ordered_words = np.argsort(sims)[::-1]\n",
        "    desc_list = [tokenizer.sequences_to_texts([[w]])[0] for w in ordered_words[:10]]\n",
        "    topics_to_words.append(desc_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR7yPNDXTYM4",
        "colab_type": "code",
        "outputId": "8212bd56-0c94-4418-ed7f-fe0f2338d9a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for i, s in enumerate(topics_to_words):\n",
        "  print('Topic', i, ':', ' '.join(s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0 : способность партию вход каждого приятным выглядят впечатляет живут состоянии понять\n",
            "Topic 1 : мамам индивидуально информацию system психует доме сибирский ясно грустит подключения\n",
            "Topic 2 : приятным собственный партию рабочей показ форме feel телефон самые канала\n",
            "Topic 3 : джонсон спинка мед христе военнослужащие позвоните шарфы kill повторить крошку\n",
            "Topic 4 : секцию месяцев емес сиять спинка арсенале колпачки закончить стоянке руководством\n",
            "Topic 5 : прививки пипец телефон америку связи предмет летний изделия cobra технологии\n",
            "Topic 6 : эксклюзивный подробности днепр сразу помощью работы опыт сухой cobra спустился\n",
            "Topic 7 : взгляд пётр system картона ненавидишь родственники использовался доме песком лишь\n",
            "Topic 8 : победа park хвост ситуациях русские парке прийти рака вашему утром\n",
            "Topic 9 : индивидуально голове снижена вашим снова доме планировка информацию неудачу иисусе\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM-mE6fK-IQf",
        "colab_type": "text"
      },
      "source": [
        "Результат получился не очень хорошим. Не получается понять о чём именно говорится в каком-либо топике"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdLwh7BgQpWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.Series(topics_to_words, name='label').apply(' '.join).\\\n",
        "  to_csv('/gdrive/My Drive/ITMO/ABAE_topics_labels.csv', header=True, index_label='label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8epFy3K-Ibe",
        "colab_type": "text"
      },
      "source": [
        "Проверим окончательно, для этого пропустим тестовые данные и попробуем отнести "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AadiIB9RKTce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_test = tokenizer.texts_to_sequences(test_df['text'])\n",
        "encoded_test = pad_sequences(encoded_test, maxlen=max_sentence_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ScHmOxQI4R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_fn = K.function([model.get_layer('sentence_input').input, K.learning_phase()], \n",
        "        [model.get_layer('p_t').output])\n",
        "\n",
        "aspect_probs = []\n",
        "for i in range(0, len(encoded_test), 100):\n",
        "  batch_probs = test_fn([encoded_test[i:i+100], 1])[0]\n",
        "  aspect_probs.extend(batch_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-OFEfJyKcpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df['label'] = np.argmax(aspect_probs, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3TME0XoPXci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df['label'].to_csv('/gdrive/My Drive/ITMO/ABAE_labels.csv', header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sSNXzQGSOoi",
        "colab_type": "text"
      },
      "source": [
        "Результат плохой, модель в основном распределила тестовые данные на 4 кластера:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoABFVqQTDgu",
        "colab_type": "code",
        "outputId": "85756c7a-742e-4534-dcfa-4bae251fefbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "test_df.groupby('label').size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    2   \n",
              "2    1   \n",
              "6    354 \n",
              "8    7743\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmYHBsudWrHR",
        "colab_type": "code",
        "outputId": "ffebc53c-36cb-45ae-9824-151d0092a57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "test_df[test_df['label'] == 6].tail(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8085</th>\n",
              "      <td>чочо упячка упячка упячка прдунь прдунь чаке страшне гневе попячтесь онотоле серчае отаке воене чочо упячка упячка упячка чочо упячка упячка упячка ебани стыд ояебу ебанулись отличненько онотоле проклинае попячтесь ебани стыд балетячка балетячка смотри сука ояебу ебанулись отличненько потс зохвачне ъеччожа попячтс богатове онотоле кагбе следит свобода равенство упячка онотоле серчае жепь ебрило онотолей пыщщщщщщ слава леониде воене упчк страшне отаке ддосе онотоле серчае свобода равенство упячка отаке воене онотоле негодуе онотоле серчае эекстелр тыой яебанейу гландэ ояебу попячтс попячтс слава чаке эекстелр тыой яебанейу потсь глагне смотри балет сука попячтс богатове пробовал лизать октаэдр ояебу адинадин онотолей онотолей медведев подсветка авто изменение подсветки приборной панели изготовление световых логотипов авто подсветка интерьера экстерьера светодиодами неоном установка магнитол сабиков усилков ксенона доступные цены высокое качество работы годовая гарантия сроки эксплуатации пяти лет находитесь другом городе вышлем</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8088</th>\n",
              "      <td>мая года центральный аэродром имени фрунзе приземлился самолет экипажем алексея ивановича семенкова доставивший москву акт безоговорочной капитуляции фашистской германии война длилась четыре года стала самым крупным вооруженным столкновением истории человечества фронте простиравшемся баренцева черного морей обеих сторон различные периоды сражались млн поздравляем днем победы президент россии владимир путин ходе парада победы заявил страна позволит переписать историю память победе фашизмом великой отечественной войне путин заявил россия позволит переписать историю великой отечественной войны сегодня подвиг народа спасшего мир рабства истребления ужасов холокоста пытаются перечеркнуть исказить события войны предать забвению подлинных героев подделать переписать переврать историю позволим сделать долг хранить память доблести воинов отдавших жизни ради жизни других солдатах мужественных бойцах второго фронта вкладе победу стран антигитлеровской коалиции боевом братстве противостоял нацизму заявил путин глава государства подчеркнул советский народ дрогнул прогнулся жестоким врагом некоторые государства предпочли позор капитуляции лицемерного соглашательства прямого сотрудничества нацистами</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  label\n",
              "doc_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
              "8085    чочо упячка упячка упячка прдунь прдунь чаке страшне гневе попячтесь онотоле серчае отаке воене чочо упячка упячка упячка чочо упячка упячка упячка ебани стыд ояебу ебанулись отличненько онотоле проклинае попячтесь ебани стыд балетячка балетячка смотри сука ояебу ебанулись отличненько потс зохвачне ъеччожа попячтс богатове онотоле кагбе следит свобода равенство упячка онотоле серчае жепь ебрило онотолей пыщщщщщщ слава леониде воене упчк страшне отаке ддосе онотоле серчае свобода равенство упячка отаке воене онотоле негодуе онотоле серчае эекстелр тыой яебанейу гландэ ояебу попячтс попячтс слава чаке эекстелр тыой яебанейу потсь глагне смотри балет сука попячтс богатове пробовал лизать октаэдр ояебу адинадин онотолей онотолей медведев подсветка авто изменение подсветки приборной панели изготовление световых логотипов авто подсветка интерьера экстерьера светодиодами неоном установка магнитол сабиков усилков ксенона доступные цены высокое качество работы годовая гарантия сроки эксплуатации пяти лет находитесь другом городе вышлем                                                                                                                                                                   6    \n",
              "8088    мая года центральный аэродром имени фрунзе приземлился самолет экипажем алексея ивановича семенкова доставивший москву акт безоговорочной капитуляции фашистской германии война длилась четыре года стала самым крупным вооруженным столкновением истории человечества фронте простиравшемся баренцева черного морей обеих сторон различные периоды сражались млн поздравляем днем победы президент россии владимир путин ходе парада победы заявил страна позволит переписать историю память победе фашизмом великой отечественной войне путин заявил россия позволит переписать историю великой отечественной войны сегодня подвиг народа спасшего мир рабства истребления ужасов холокоста пытаются перечеркнуть исказить события войны предать забвению подлинных героев подделать переписать переврать историю позволим сделать долг хранить память доблести воинов отдавших жизни ради жизни других солдатах мужественных бойцах второго фронта вкладе победу стран антигитлеровской коалиции боевом братстве противостоял нацизму заявил путин глава государства подчеркнул советский народ дрогнул прогнулся жестоким врагом некоторые государства предпочли позор капитуляции лицемерного соглашательства прямого сотрудничества нацистами  6    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiDRwlBKPXqS",
        "colab_type": "code",
        "outputId": "40b4e58e-b794-4791-a527-cec6ecc5ffd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "test_df[test_df['label'] == 0].tail(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3738</th>\n",
              "      <td>ստեղ սեր կոչվածը հոմանիշ ատելությանը ստեղ սիրահարները թշնամի իսկ թշնամիները համբուրվում ստեղ ամեն ինչ խառն ստրուկն արքա իսկ արքան բեռնաձի ստեղ տարածքում պոետները ոռնում ցավում լացում տանջվում ստեղ պոետները մեռնում որովհետև ստեղ ամեն ինչ խառն կարևորն այն ինչ ասաց այլ այն ինչպես ասաց սոմերսեթ մոեմ լուսին վեցպենսանոց ժամանակ դուռը միշտ կողպած պահելու սովորությունը լրացուցիչ հմայք ուներ միշտ գալիս դուռը քեզ համար բացելու հիմա բաց պահում ինքդ մտնես չեմ դիմավորում հեռախոսս արդեն թողնում սենյակում ուղղակի չեմ լսել ինչպես զանգել այդպես գոնե ինձ եթե քեզ խաբել կլինի հիմա վերնաշապիկներդ անուններ ունեն սպիտակը կապույտը գծավորը առաջ դրանք դիտմամբ անանուն էին գայի քեզ մոտ անպայման մոտիկից հարցնեի որն ուզում արդուկեմ անուններով կողքի սենյակից կարելի ճշտել մատանին ձեռնաշղթայի նմանվել այն մատանին լվացվելիս քնելիս չէի հանում դու էիր ախր այն հագցրել մատիս հիմա անսովոր թվում արհեստական ավելորդ գույնն անգամ այլ թվում նկարի շրջանակ չեմ գնում նոր նկարներ չենք անում հները տպել չեմ ուզում առանց հարցնելու երկու բաժակ սուրճ դնելու փոխարեն քեզ հարցնում կխմես հարցնում բաժակ լինի դուռը բաց թողնելով վերնաշապիկներին անուն տալով կենտ սուրճով քանդեցինք լիլի մկրտչյան</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6246</th>\n",
              "      <td>memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably πятничкa memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably лaйфхaк memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably рoдитeли пoймут memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  label\n",
              "doc_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
              "3738    ստեղ սեր կոչվածը հոմանիշ ատելությանը ստեղ սիրահարները թշնամի իսկ թշնամիները համբուրվում ստեղ ամեն ինչ խառն ստրուկն արքա իսկ արքան բեռնաձի ստեղ տարածքում պոետները ոռնում ցավում լացում տանջվում ստեղ պոետները մեռնում որովհետև ստեղ ամեն ինչ խառն կարևորն այն ինչ ասաց այլ այն ինչպես ասաց սոմերսեթ մոեմ լուսին վեցպենսանոց ժամանակ դուռը միշտ կողպած պահելու սովորությունը լրացուցիչ հմայք ուներ միշտ գալիս դուռը քեզ համար բացելու հիմա բաց պահում ինքդ մտնես չեմ դիմավորում հեռախոսս արդեն թողնում սենյակում ուղղակի չեմ լսել ինչպես զանգել այդպես գոնե ինձ եթե քեզ խաբել կլինի հիմա վերնաշապիկներդ անուններ ունեն սպիտակը կապույտը գծավորը առաջ դրանք դիտմամբ անանուն էին գայի քեզ մոտ անպայման մոտիկից հարցնեի որն ուզում արդուկեմ անուններով կողքի սենյակից կարելի ճշտել մատանին ձեռնաշղթայի նմանվել այն մատանին լվացվելիս քնելիս չէի հանում դու էիր ախր այն հագցրել մատիս հիմա անսովոր թվում արհեստական ավելորդ գույնն անգամ այլ թվում նկարի շրջանակ չեմ գնում նոր նկարներ չենք անում հները տպել չեմ ուզում առանց հարցնելու երկու բաժակ սուրճ դնելու փոխարեն քեզ հարցնում կխմես հարցնում բաժակ լինի դուռը բաց թողնելով վերնաշապիկներին անուն տալով կենտ սուրճով քանդեցինք լիլի մկրտչյան                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0    \n",
              "6246    memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably πятничкa memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably лaйфхaк memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably рoдитeли пoймут memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably memorably  0    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7V_31tkSyTZ",
        "colab_type": "text"
      },
      "source": [
        "Данная модель показывает себя очень плохо - в 8 кластере оказываются почти все документы. Но и в остальных кластерах не видно похожих текстов"
      ]
    }
  ]
}